{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# What's Cooking Challenge\n",
    "\n",
    "### <i> Progetto per l'esame di Programmazione di applicazioni di Data Intensive (2019) </i>\n",
    "\n",
    "### Cichetti Federico, Sponziello Nicolò\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il progetto ha lo scopo di creare un modello in grado di classificare il tipo di cucina di una ricetta in base agli ingredienti forniti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esplorazione dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partiamo caricando i dati in un dataframe Pandas e visualizzandone una parte per capire come sono strutturati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"train.json\")\n",
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il dataframe ha tre colonne:\n",
    "- \"cuisine\" indica il tipo di cucina a cui appartiene il piatto. Questa sarà l'incognita da scoprire.\n",
    "- \"id\" è una colonna che contiene un numero identificativo di ogni piatto. Questo dato non è utile al problema, quindi decidiamo di eliminare la colonna e usare come identificativo l'indice aggiunto in automatico da Pandas.\n",
    "- \"ingredients\" contiene la lista di ingredienti del piatto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"id\" in df:\n",
    "    df.drop(\"id\", inplace=True, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prima di procedere si esplorano e visualizzano alcuni dati, in particolare:\n",
    "* Quante ricette sono presenti\n",
    "* La totalità degli ingredienti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le ricette da classificare nel dataset sono 39774 in totale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['cuisine'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In totale ci sono 20 tipi di cucine differenti. Si tratta quindi di un problema di classificazione multiclasse.\n",
    "Controlliamo quanti piatti ci sono per ogni cucina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['cuisine'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cuisine'].value_counts().plot(\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dal grafico possiamo notare che le classi sono sbilanciate, cioè sono presenti molte ricette che vengono identificate come cucina italiana e messicana, mentre ci sono poche ricette russe e brasiliane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per analizzare gli ingredienti, per adesso usiamo un approccio iterativo. Si crea un set di ingredienti in modo da eliminare eventuali duplicati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = set()\n",
    "for recipe in df['ingredients']:\n",
    "    for i in recipe:\n",
    "        ingredients.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ingredients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In totale notiamo che in tutto ci sono 6714 ingredienti diversi nel dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizziamo quelli più usati. Per fare questo gli ingredienti vanno inseriti in una lista in modo da mantenere i duplicati che poi dovranno essere contati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_list = list()\n",
    "for i in df['ingredients']:\n",
    "    for word in i:\n",
    "        ingredients_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_ingredients = pd.Series(ingredients_list)\n",
    "common_ingredients.value_counts().nlargest(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questi sono gli ingredienti più comuni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_ingredients.value_counts().nlargest(20).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizziamo ora gli ingredienti più comuni per ogni cucina. Si crea un dizionario che contiene per ogni cucina un dizionario ingrediente -> numero di occorrenze e lo si ordina per tale conteggio. Possiamo poi costruire un DataFrame per visualizzare efficacemente quali sono gli ingredienti più usati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.groupby('cuisine')['ingredients'].apply(list)\n",
    "\n",
    "def most_common_ingr_by_cuisine(cuisine):\n",
    "    lists = tmp[cuisine]\n",
    "    res = defaultdict(int)\n",
    "    for recipe in lists:\n",
    "        for ingr in recipe:\n",
    "            res[ingr] += 1\n",
    "    return sorted(res.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chiamando la funzione qui sopra passando, ad esempio, la classe \"italian\" possiamo vedere che sale, olio di oliva, aglio e parmigiano sono alcuni degli ingredienti più comuni della cucina italiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commons_ital = pd.DataFrame(most_common_ingr_by_cuisine('italian'), columns=[\"ingredient\", \"count\"]).head(10)\n",
    "commons_ital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In realtà il sale è l'ingrediente più comune per molte cucine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(most_common_ingr_by_cuisine('french'), columns=[\"ingredient\", \"count\"]).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(most_common_ingr_by_cuisine('brazilian'), columns=[\"ingredient\", \"count\"]).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo ora una funzione che restituisca il numero di ingredienti medio per ogni ricetta di una determinata cucina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_ingr_per_recipe(cuisine):\n",
    "    recipes = tmp[cuisine]\n",
    "    count = 0;\n",
    "    for recipe in recipes:\n",
    "        for ingr in recipe:\n",
    "            count += 1;\n",
    "    return count/len(recipes)\n",
    "\n",
    "avg_ingr_per_recipe('brazilian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average = {cuisine: avg_ingr_per_recipe(cuisine) for cuisine in df['cuisine'].unique()}\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(21, 3))\n",
    "plt.bar(average.keys(), average.values(), align=\"center\", width=0.5)\n",
    "plt.title(\"Ingredienti medi per ricetta per ogni cucina\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questa fase, partendo dai risultati dell'analisi, effettuiamo una trasformazione dei dati in modo che siano pronti per essere elaborati dagli algoritmi di learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come abbiamo potuto notare, il dataframe si compone di righe formate da:\n",
    "\n",
    "- **[cuisine]**: categoria di cucina\n",
    "- **[ingredients]**: lista di ingredienti in formato testuale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una prima domanda che ci si può porre è come gestire gli ingredienti di una ricetta dato che spesso, come emerso dall'analisi dei dati, sono composti da più parole.\n",
    "- Una soluzione possibile è quella di considerarli cosi come sono presenti, cioè \"black olives\" rimarrebbe \"black olives\"\n",
    "- Oppure si potrebbero unire tutti gli ingredienti di una ricetta in un'unica stringa e applicare tecniche di text processing per cercare di estrarre più informazioni possibili\n",
    "    - Stemming, Lemmatization, Bag of Word, Vector Space Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarizzazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un primo approccio naive sarebbe quello di estrarre dal dataset un set contenente tutti gli ingredienti presenti e binarizzare ogni ricetta.\n",
    "- Ad ogni riga, che rappresenta una ricetta, è associato un vettore di elementi in [0, 1] in cui la cella corrispondente all'ingrediente contiene 1 se è usato nella ricetta, 0 altrimenti.\n",
    "- Avremmo ottenuto un dataset con circa 6700 features e 40'000 istanze, con un'alta occupazione di memoria e lunghi tempi di calcolo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oltre ad essere evidentemente un metodo poco scalabile, lento, e che occupa livelli veramente alti di memoria, lavora considerando troppi ingredienti e probabilmente è troppo complesso per essere efficace. In seguito ad alcuni test non riportati, si è effettivamente rivelato abbastanza inefficiente e inaccurato, motivo per cui sono stati subito scelti approcci differenti, previlegiando soluzioni con un numero di feature ridotto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo modello consente di rappresentare le ricette come vettori all'interno di un iperspazio in cui a ogni singola parola del dizionario comune (in questo caso, ogni singolo ingrediente) viene associato un peso che indica quanto esso contraddistingua la ricetta stessa.\n",
    "Il modello tf-idf si adatta particolarmente bene allo scopo, in quanto:\n",
    "- Associa ad ogni parola un peso che dipende sia dalla frequenza di uso locale Tf (cioè nella stessa ricetta) sia negli altri documenti idf (ricette)\n",
    "- Tutti gli ingredienti vengono mappati\n",
    "- I valori per ogni parola sono normalizzati in [0, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di default, un TfidfVectorizer() effettua del preprocessing prima di creare la matrice che contiene i pesi. Questo preprocessing può comprendere l'eliminazione di eventuali stopword (cioè parole inglesi di poca importanza nella comprensione dei contenuti che vengono scartate), segni di punteggiatura e tanto altro. Una possibile variante è quella di considerare le ricette sia divise per singole parole che per n-uple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per prima cosa, manipoliamo la colonna 'ingredients' del dataframe\n",
    "- trasformiamo la lista di ingredienti in un'unica stringa in modo che possa essere letta dal tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in df.index:\n",
    "    txt = \"\"\n",
    "    for ing in df.loc[idx, \"ingredients\"]:\n",
    "        txt += (ing + \" \")\n",
    "    df.loc[idx, \"ingredients\"] = txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo i set per il training dei modelli e il calcolo dello score sul validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t, X_v, y_t, y_v = train_test_split(df['ingredients'], df['cuisine'], random_state=42, test_size=1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniziamo lo studio dei modelli di classificazione partendo dal più semplice: il Perceptron. Con class_weight=\"balanced\" si fa un tentativo di bilanciare le classi dato che come si è visto ci sono molte più ricette italiane e mesicane che altre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"perc\", Perceptron())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per ottimizzare i parametri, usiamo la Grid Search\n",
    " - Testiamo anche quale ngram ottimizza lo score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid1 = {\n",
    "    'tfidf__ngram_range':[(1, 1), (1, 2), (1, 3)],  #considerazioni di unigrammi, bigrammi, ...\n",
    "    'perc__penalty': [None, 'l1', 'l2'], #regolarizzazioni da effettuare\n",
    "    'perc__alpha': np.logspace(-5, -3, 3),\n",
    "    'perc__class_weight': [None, \"balanced\"]\n",
    "}\n",
    "gs_perc = GridSearchCV(perceptron, param_grid=grid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7131543219188414"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_perc.fit(X_t, y_t)\n",
    "gs_perc.score(X_v, y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_perc__alpha</th>\n",
       "      <th>param_perc__class_weight</th>\n",
       "      <th>param_perc__penalty</th>\n",
       "      <th>param_tfidf__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.649970</td>\n",
       "      <td>0.115817</td>\n",
       "      <td>0.481009</td>\n",
       "      <td>0.022349</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>None</td>\n",
       "      <td>l2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'perc__alpha': 1e-05, 'perc__class_weight': None, 'perc__penalty': 'l2', 'tfidf__ngram_range': (1, 2)}</td>\n",
       "      <td>0.707485</td>\n",
       "      <td>0.707466</td>\n",
       "      <td>0.712862</td>\n",
       "      <td>0.709270</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.905108</td>\n",
       "      <td>0.042470</td>\n",
       "      <td>0.741347</td>\n",
       "      <td>0.027372</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>None</td>\n",
       "      <td>l2</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{'perc__alpha': 1e-05, 'perc__class_weight': None, 'perc__penalty': 'l2', 'tfidf__ngram_range': (1, 3)}</td>\n",
       "      <td>0.709634</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.704031</td>\n",
       "      <td>0.707120</td>\n",
       "      <td>0.002323</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.595471</td>\n",
       "      <td>0.049604</td>\n",
       "      <td>0.449922</td>\n",
       "      <td>0.004231</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>balanced</td>\n",
       "      <td>l2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'perc__alpha': 1e-05, 'perc__class_weight': 'balanced', 'perc__penalty': 'l2', 'tfidf__ngram_range': (1, 2)}</td>\n",
       "      <td>0.700814</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.700408</td>\n",
       "      <td>0.704254</td>\n",
       "      <td>0.005154</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.659565</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>0.665171</td>\n",
       "      <td>0.014375</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>balanced</td>\n",
       "      <td>l2</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{'perc__alpha': 1e-05, 'perc__class_weight': 'balanced', 'perc__penalty': 'l2', 'tfidf__ngram_range': (1, 3)}</td>\n",
       "      <td>0.699231</td>\n",
       "      <td>0.711652</td>\n",
       "      <td>0.695879</td>\n",
       "      <td>0.702255</td>\n",
       "      <td>0.006784</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.652189</td>\n",
       "      <td>0.014949</td>\n",
       "      <td>0.180327</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>None</td>\n",
       "      <td>l2</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'perc__alpha': 1e-05, 'perc__class_weight': None, 'perc__penalty': 'l2', 'tfidf__ngram_range': (1, 1)}</td>\n",
       "      <td>0.699683</td>\n",
       "      <td>0.701697</td>\n",
       "      <td>0.695992</td>\n",
       "      <td>0.699125</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "4        1.649970      0.115817         0.481009        0.022349   \n",
       "5        2.905108      0.042470         0.741347        0.027372   \n",
       "10       1.595471      0.049604         0.449922        0.004231   \n",
       "11       2.659565      0.002885         0.665171        0.014375   \n",
       "3        0.652189      0.014949         0.180327        0.001539   \n",
       "\n",
       "   param_perc__alpha param_perc__class_weight param_perc__penalty  \\\n",
       "4              1e-05                     None                  l2   \n",
       "5              1e-05                     None                  l2   \n",
       "10             1e-05                 balanced                  l2   \n",
       "11             1e-05                 balanced                  l2   \n",
       "3              1e-05                     None                  l2   \n",
       "\n",
       "   param_tfidf__ngram_range  \\\n",
       "4                    (1, 2)   \n",
       "5                    (1, 3)   \n",
       "10                   (1, 2)   \n",
       "11                   (1, 3)   \n",
       "3                    (1, 1)   \n",
       "\n",
       "                                                                                                           params  \\\n",
       "4         {'perc__alpha': 1e-05, 'perc__class_weight': None, 'perc__penalty': 'l2', 'tfidf__ngram_range': (1, 2)}   \n",
       "5         {'perc__alpha': 1e-05, 'perc__class_weight': None, 'perc__penalty': 'l2', 'tfidf__ngram_range': (1, 3)}   \n",
       "10  {'perc__alpha': 1e-05, 'perc__class_weight': 'balanced', 'perc__penalty': 'l2', 'tfidf__ngram_range': (1, 2)}   \n",
       "11  {'perc__alpha': 1e-05, 'perc__class_weight': 'balanced', 'perc__penalty': 'l2', 'tfidf__ngram_range': (1, 3)}   \n",
       "3         {'perc__alpha': 1e-05, 'perc__class_weight': None, 'perc__penalty': 'l2', 'tfidf__ngram_range': (1, 1)}   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  mean_test_score  \\\n",
       "4            0.707485           0.707466           0.712862         0.709270   \n",
       "5            0.709634           0.707692           0.704031         0.707120   \n",
       "10           0.700814           0.711538           0.700408         0.704254   \n",
       "11           0.699231           0.711652           0.695879         0.702255   \n",
       "3            0.699683           0.701697           0.695992         0.699125   \n",
       "\n",
       "    std_test_score  rank_test_score  \n",
       "4         0.002539                1  \n",
       "5         0.002323                2  \n",
       "10        0.005154                3  \n",
       "11        0.006784                4  \n",
       "3         0.002362                5  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gs_perc.cv_results_).sort_values(\"rank_test_score\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si salva il modello su disco:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_perc.bin\", \"wb\") as f:\n",
    "    pickle.dump(gs_perc.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note riguardo al perceptron\n",
    "\n",
    "Questo modello è già abbastanza buono, ma probabilmente si può fare di meglio. A quanto pare non serve inserire \"balanced\" come class weight - proveremo un metodo di bilanciamento più complesso più avanti. Il peso della regolarizzazione deve essere piuttosto piccolo e si preferisce una regolarizzazione di tipo L2.\n",
    "Proviamo l'accuratezza di un modello casuale per vedere quanto si discosta da quella rilevata con il nostro modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        italian\n",
       "1        mexican\n",
       "2    southern_us\n",
       "3         indian\n",
       "4        chinese\n",
       "dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuisines = pd.Series(df[\"cuisine\"].value_counts().keys(), index=range(0, 20, 1))\n",
    "cuisines.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True, ['thai', 'italian', 'chinese', 'irish', 'filipino']\n"
     ]
    }
   ],
   "source": [
    "random_predictions = [cuisines.iloc[random.randint(0, 19)] for i in range(0, len(X_v))]\n",
    "print(str(len(random_predictions) == len(X_v)) + \", \" + str(random_predictions[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05219490119173329"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran_ser = pd.Series(random_predictions)\n",
    "len(ran_ser[ran_ser == pd.Series(y_v.values)]) / len(X_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'accuratezza del circa 5% era attesa dato che si tratta di selezionare uno tra venti valori differenti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miglioramento dei risultati (Regressione Logistica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo a migliorare il risultato:\n",
    "- Preprocessando in maniera differente i dati\n",
    "- Utilizzando modelli più complessi e accurati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo una nostra funzione di preprocessing per avere più controllo su questa fase. Il processamento di un ingrediente viene fatto attraverso i seguenti passaggi:\n",
    "- tokenizzazione per dividere in singole parole gli ingredienti multiparola\n",
    "- casefolding\n",
    "- rimozione delle stopwords e delle parole non-alfabetiche per sfoltire l'insieme degli ingredienti\n",
    "- rimozione delle unità di misura\n",
    "- stemming/lemmatization dei token ottenuti con i passaggi precedenti in modo da estrarre solamente la parte rilveante degli stessi (la loro radice morfologica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = ['oz', 'gram', 'g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stem(ingredients):\n",
    "    #tokenization\n",
    "    tokens = nltk.tokenize.word_tokenize(ingredients)\n",
    "    #token filtering\n",
    "    tokens_left = {token.lower() for token in tokens} #casefolding\n",
    "    for tok in set(tokens): #do not check duplicates\n",
    "        if tok in nltk.corpus.stopwords.words(\"english\") or not tok.isalpha() or tok in units: #non-alphabetic words and stopwords removal\n",
    "            tokens_left.remove(tok)\n",
    "    #stemming\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    stemmed_tokens = {ps.stem(tok) for tok in tokens_left}\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lem(ingredients):\n",
    "    tokens = {token.lower() for token in nltk.tokenize.word_tokenize(ingredients)}\n",
    "    for tok in tokens:\n",
    "        if tok in nltk.corpus.stopwords.words(\"english\") or not tok.isalpha() or tok in units:\n",
    "            tokens.remove(tok)\n",
    "#Lemmatization\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "return {lemmatizer.lemmatize(tok) for tok in tokens}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo ora a testare questa funzione insieme a quella di default in una Grid Search in cui si utilizza come modello la Regressione Logistica. Mettiamo anche alcuni iperparametri della regressione nella Grid Search per ottenere il miglior modello possibile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2 = {\n",
    "    'model__C' : [1, 5, 10],\n",
    "    'tokenizer__tokenizer' : [None, preprocess_stem, preprocess_lem]\n",
    "}\n",
    "logreg = Pipeline([\n",
    "    (\"tokenizer\", TfidfVectorizer()),\n",
    "    (\"model\",  LogisticRegression()) #se va male: fit_intercept=False\n",
    "])\n",
    "gs_logreg = GridSearchCV(logreg, param_grid=grid2)\n",
    "gs_logreg.fit(X_t, y_t)\n",
    "gs_logreg.score(X_v, y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gs_logreg.cv_results_).sort_values(\"rank_test_score\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Descrizione dei risultati]\n",
    "La funzione di preprocessing potrebbe non essere necessaria (provare con lemmatization?)\n",
    "Migliori parametri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonostante il preprocessing potrebbe non essere ottimale, il modello è più accurato quindi possiamo cercare di capire che cosa ha imparato controllando i parametri.\n",
    "Possiamo creare un DataFrame che associ a ogni cucina per tutte le parole del dizionario individuate dal Vectorizer il corrispondente peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logreg = gs_logreg.best_estimator_\n",
    "coeffs = pd.DataFrame(model_logreg.named_steps[\"model\"].coef_, index=model_logreg.classes_, columns=model_logreg.named_steps[\"tokenizer\"].get_feature_names())\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo modo selezionando una cucina e un ingrediente si può vedere il peso assegnato all'interno di una eventuale query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs.loc[\"italian\", \"mozzarella\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs.loc[\"mexican\", \"corn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo inoltre studiare quali siano gli ingredienti più caratteristici di ogni cucina secondo questi pesi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coeffs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-3c4384ec1e17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m best_ing_data = {\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"cuisine\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"cuisine\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;34m\"best_ingredient\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcuisine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midxmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcuisine\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"cuisine\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;34m\"score\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcuisine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcuisine\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"cuisine\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m }\n",
      "\u001b[1;32m<ipython-input-50-3c4384ec1e17>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m best_ing_data = {\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"cuisine\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"cuisine\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;34m\"best_ingredient\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcuisine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midxmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcuisine\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"cuisine\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;34m\"score\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcuisine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcuisine\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"cuisine\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'coeffs' is not defined"
     ]
    }
   ],
   "source": [
    "best_ing_data = {\n",
    "    \"cuisine\" : df[\"cuisine\"].unique(),\n",
    "    \"best_ingredient\" : [coeffs.loc[cuisine, :].idxmax() for cuisine in df[\"cuisine\"].unique()],\n",
    "    \"score\" : [coeffs.loc[cuisine, :].max() for cuisine in df[\"cuisine\"].unique()]\n",
    "}\n",
    "best_ingredients = pd.DataFrame(best_ing_data).set_index([\"cuisine\"])\n",
    "best_ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nota già da questo DataFrame che gli ingredienti più caratteristici di alcune cucine sono semplicemente gli aggettivi che ne indicano le provenienze (russian, irish), nonostante non siano veri e propri ingredienti. Per le altre cucine, invece, gli ingredienti principali sono abbastanza realistici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo ancora a migliorare i risultati utilizzando un modello ancora più complesso: SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ma prima, salviamo anche questo modello su disco:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_logreg.bin\", \"wb\") as f:\n",
    "    pickle.dump(gs_logreg.best_estimator, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le Support Vector Machines sono uno strumento molto potente per individuare iperpiani di separazione ottimi, ovvero quelli che generano minore overfitting. Invece di considerare tutte le istanze, si considerano solo quelle vicine al decision boundary, i cosiddetti Support Vector e si cerca di massimizzare la distanza tra questi punti e l'iperpiano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gli iperparametri per SVM sono i seguenti:\n",
    "- il tipo di funzione kernel da usare\n",
    "- il grado della funzione kernel\n",
    "- C, ovvero un parametro che controlla l'overfitting effettuando uno spostamento dei dati misclassified\n",
    "- gamma, ovvero un parametro che controlla quanto il decision boundary sia flessibile (in rbf corrisponde all'ampiezza della gaussiana)\n",
    "\n",
    "Abilitiamo anche probability=True per abilitare l'utilizzo di predict_proba nel modello finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid3 = {\n",
    "    'model__gamma' : [0.1, 1, 5],\n",
    "    'model__C' : [0.1, 1, 5],\n",
    "    'model__kernel' : ['rbf', 'poly']\n",
    "}\n",
    "SVC = Pipeline([\n",
    "    (\"tokenizer\", TfidfVectorizer()),\n",
    "    (\"model\",  SVC(probability=True))\n",
    "])\n",
    "gs_SVC = GridSearchCV(SVC, grid3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_SVC.fit(X_t, y_t)\n",
    "gs_SVC.score(X_v, y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gs_SVC.cv_results_).sort_values(\"rank_test_score\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[conclusioni su questo metodo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ancora una volta, salviamo il modello su disco:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_SVM.bin\", \"wb\") as f:\n",
    "    pickle.dump(gs_SVC.best_estimator, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affidabilità dei modelli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vuole calcolare quanto i modelli trovati finora siano affidabili. Per farlo utilizzeremo diverse metriche:\n",
    "- Si calcola la matrice di confusione\n",
    "- Si calcolano precision, recall e f1-score dei modelli\n",
    "- Calcolo degli intervalli di confidenza con confidenza fissata al 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_calculation(model):\n",
    "    return confusion_matrix(y_v, model.predict(X_v))\n",
    "\n",
    "def calculate_precision_recall_f1(model):\n",
    "    y_v_predictions = model.predict(X_v)\n",
    "    p = precision_score(y_v, y_v_predictions, pos_label=1, average=\"macro\")\n",
    "    r = recall_score(y_v, y_v_predictions, average=\"macro\")\n",
    "    f1 = f1_score(y_v, y_v_predictions, average=\"macro\")\n",
    "    return {\"precision\" : p, \"recall\": r, \"f1-score\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>greek</th>\n",
       "      <th>southern_us</th>\n",
       "      <th>filipino</th>\n",
       "      <th>indian</th>\n",
       "      <th>jamaican</th>\n",
       "      <th>spanish</th>\n",
       "      <th>italian</th>\n",
       "      <th>mexican</th>\n",
       "      <th>chinese</th>\n",
       "      <th>british</th>\n",
       "      <th>thai</th>\n",
       "      <th>vietnamese</th>\n",
       "      <th>cajun_creole</th>\n",
       "      <th>brazilian</th>\n",
       "      <th>french</th>\n",
       "      <th>japanese</th>\n",
       "      <th>irish</th>\n",
       "      <th>korean</th>\n",
       "      <th>moroccan</th>\n",
       "      <th>russian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>greek</th>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>southern_us</th>\n",
       "      <td>1</td>\n",
       "      <td>109</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filipino</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>335</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indian</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>610</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>51</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jamaican</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>121</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spanish</th>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>515</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>137</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>61</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>265</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexican</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>812</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chinese</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>british</th>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>131</td>\n",
       "      <td>81</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>2092</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>61</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thai</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vietnamese</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>331</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cajun_creole</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>198</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brazilian</th>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1884</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>61</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>french</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>175</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>japanese</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>irish</th>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>76</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>61</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>85</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>1009</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>korean</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moroccan</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>382</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>russian</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              greek  southern_us  filipino  indian  jamaican  spanish  \\\n",
       "greek            92            2         5       1         9        2   \n",
       "southern_us       1          109         2       3         0       34   \n",
       "filipino          2            7       335       1         3       17   \n",
       "indian            2            0         8     610        22        9   \n",
       "jamaican          6            1         5      21       121        5   \n",
       "spanish           2           44        14       2         6      515   \n",
       "italian           0            6         0       0         0       18   \n",
       "mexican           7            7         3       4         7       12   \n",
       "chinese           0           31         2       2         0       36   \n",
       "british           6           23        29      12         7      131   \n",
       "thai              5            5         2       0         4        6   \n",
       "vietnamese        1            5         5      34         7        6   \n",
       "cajun_creole      0            1         1      30         1        3   \n",
       "brazilian        12            4        11       9         5       21   \n",
       "french            0            1         0       1         1       11   \n",
       "japanese          0           12         1       6         0       22   \n",
       "irish             4           26        76       8         6       61   \n",
       "korean            1            3         8       0         2       35   \n",
       "moroccan          6            1         0      17        11        0   \n",
       "russian           0            3         1      24         8        6   \n",
       "\n",
       "              italian  mexican  chinese  british  thai  vietnamese  \\\n",
       "greek               0        2        3       13     1           1   \n",
       "southern_us         1        6       21       21     3           3   \n",
       "filipino           11        0        4       24     5           0   \n",
       "indian              5        1        1       20     4          51   \n",
       "jamaican            0        1        5        9     2           6   \n",
       "spanish            15        4       25      137     4           1   \n",
       "italian           265        3        1       55     1           0   \n",
       "mexican            19      812        3       19     5           9   \n",
       "chinese             5        0       83       23     2           2   \n",
       "british            81        8       16     2092     6           4   \n",
       "thai                0        2        3        3    91           3   \n",
       "vietnamese          3       43        3       10     1         331   \n",
       "cajun_creole        0        0        0        7     1          20   \n",
       "brazilian          15        6        4       34     5           6   \n",
       "french             16       18        0        9     2           0   \n",
       "japanese            8        0        2       20     1           2   \n",
       "irish               8        3       27       85     9          11   \n",
       "korean             11        1        1       37     1           2   \n",
       "moroccan            4       22        1        6     3          10   \n",
       "russian             1        2        0        2     3          11   \n",
       "\n",
       "              cajun_creole  brazilian  french  japanese  irish  korean  \\\n",
       "greek                    0         10       1         0     12      11   \n",
       "southern_us              0          1       1         9     49       6   \n",
       "filipino                 2         20       1         5     90      12   \n",
       "indian                  40         10       0         1     11       2   \n",
       "jamaican                 9         19       0         0      3       4   \n",
       "spanish                  1         19       2        15     61      52   \n",
       "italian                  0          3       9         1      8      15   \n",
       "mexican                  3         32      26         3      7       9   \n",
       "chinese                  0          7       0         3     28       2   \n",
       "british                  6         32      20         5     61      62   \n",
       "thai                     1         10       0         3     10       1   \n",
       "vietnamese              19          3       0         0     10       4   \n",
       "cajun_creole           198          3       0         0      4       1   \n",
       "brazilian                6       1884      10         8     61      41   \n",
       "french                   0          7     175         1      8      14   \n",
       "japanese                 0          6       3        51     13       7   \n",
       "irish                    6         50       1        19   1009      23   \n",
       "korean                   1         23       5         3     19     163   \n",
       "moroccan                 9          8       1         0      8       0   \n",
       "russian                  5          5       2         0      3       2   \n",
       "\n",
       "              moroccan  russian  \n",
       "greek                0        0  \n",
       "southern_us          0        1  \n",
       "filipino             0        0  \n",
       "indian              32       18  \n",
       "jamaican             3        7  \n",
       "spanish              1        1  \n",
       "italian              0        0  \n",
       "mexican             19        2  \n",
       "chinese              0        0  \n",
       "british              0        1  \n",
       "thai                 2        0  \n",
       "vietnamese           5        3  \n",
       "cajun_creole         5        2  \n",
       "brazilian            7        3  \n",
       "french               0        0  \n",
       "japanese             0        3  \n",
       "irish                6        5  \n",
       "korean               1        3  \n",
       "moroccan           382       50  \n",
       "russian             56      137  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#matrice di confusione del modello perceptron\n",
    "conf_perc = pd.DataFrame(confusion_matrix_calculation(gs_perc.best_estimator_), index=average.keys(), columns=average.keys())\n",
    "conf_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrice di confusione del modello di regressione logistica\n",
    "conf_logreg = pd.DataFrame(confusion_matrix_calculation(gs_logreg.best_estimator_), index=best_ingredients.index, columns=best_ingredients.index)\n",
    "conf_logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrice di confusione del modello SVM\n",
    "conf_SVC = pd.DataFrame(confusion_matrix_calculation(gs_SVC.best_estimator_), index=best_ingredients.index, columns=best_ingredients.index)\n",
    "conf_SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcolo delle statistiche per ogni modello trovato\n",
    "pd.DataFrame([calculate_precision_recall_f1(gs_perc.best_estimator_),\n",
    "              calculate_precision_recall_f1(gs_logreg.best_estimator_),\n",
    "              calculate_precision_recall_f1(gs_SVC.best_estimator_)],\n",
    "                 index=[\"perceptron\", \"logreg\", \"SVM\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Inserire analisi dei risultati ottenuti]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcoliamo anche gli intervalli di confidenza dei modelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence(acc, N, Z):\n",
    "    den = (2*(N+Z**2))\n",
    "    var = (Z*np.sqrt(Z**2+4*N*acc-4*N*acc**2)) / den\n",
    "    a = (2*N*acc+Z**2) / den\n",
    "    inf = a - var\n",
    "    sup = a + var\n",
    "    return (inf, sup)\n",
    "\n",
    "def calculate_accuracy(conf_matrix):\n",
    "    return np.diag(conf_matrix).sum() / conf_matrix.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7053944681458425, 0.7207906855058194)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#con confidenza del 0.95 si ha Z=1.96\n",
    "confidence(calculate_accuracy(conf_perc), len(X_v), 1.96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bilanciamento delle classi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo ora a trasformare i dati in modo che le classi siano bilanciate per vedere se i risultati che si ottengono sono migliori o peggiori. Per farlo usiamo SMOTE che fa parte del modulo imblearn che deve essere scaricato con pip con il comando \n",
    "```python\n",
    "pip3 install imblearn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "balancer = SMOTE(random_state=42)\n",
    "t = TfidfVectorizer()\n",
    "balanced_matrix = t.fit_transform(df[\"ingredients\"])\n",
    "y_cpy = df[\"cuisine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = balancer.fit_resample(balanced_matrix, y_cpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le classi sono state bilanciate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_balanced = LogisticRegression(C=5)\n",
    "X_t_b, X_v_b, y_t_b, y_v_b = train_test_split(balanced_matrix, y_cpy, random_state=42, test_size=1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_balanced.fit(X_t_b, y_t_b)\n",
    "model_balanced.score(X_v_b, y_v_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[conclusioni da fare sul modello bilanciato]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusioni [da fare]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
